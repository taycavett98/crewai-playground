# Quick Start Guide

## âœ… What's Done

All the foundational code is implemented! You can now focus on CrewAI and LangGraph.

### Implemented Components

1. **Utils** (`app/utils.py`):
   - âœ… `load_config()` - Loads YAML configuration
   - âœ… `probe_models()` - Finds intersection of supported/available models
   - âœ… `setup_logging()` - Configures application logging

2. **OllamaClient** (`app/services/ollama_client.py`):
   - âœ… Config-driven initialization (Option D pattern)
   - âœ… `list_available_models()` - Gets models from Ollama
   - âœ… `generate()` - Text generation with temperature support
   - âœ… `change_model()` - Hot-swap models at runtime
   - âœ… `health_check()` - Verify Ollama is running

3. **Pydantic Schemas** (`app/models/schemas.py`):
   - âœ… All request/response models defined
   - âœ… Proper validation and documentation

4. **API Routes**:
   - âœ… `GET /health` - Health check endpoint
   - âœ… `GET /llm/models` - List available models
   - âœ… `POST /llm/generate` - Generate text
   - âœ… `POST /llm/models/switch` - Switch models

5. **FastAPI App** (`app/main.py`):
   - âœ… All routes wired up
   - âœ… CORS enabled
   - âœ… Logging configured
   - âœ… Auto-generated docs at `/docs`

## ğŸš€ Running the API

### Option 1: Using the run script (Recommended)
```bash
source .venv/bin/activate
python run.py
```

### Option 2: Using uvicorn directly
```bash
source .venv/bin/activate
uvicorn app.main:app --reload
```

### Option 3: Using uv
```bash
uv run uvicorn app.main:app --reload
```

## ğŸ§ª Testing the API

### Interactive Docs (Best way!)
Visit: http://localhost:8000/docs

You'll see a beautiful Swagger UI where you can test all endpoints interactively!

### cURL Examples

**Health Check:**
```bash
curl http://localhost:8000/health
```

**List Models:**
```bash
curl http://localhost:8000/llm/models
```

**Generate Text:**
```bash
curl -X POST http://localhost:8000/llm/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explain what an AI agent is in one sentence"}'
```

**Switch Model:**
```bash
curl -X POST http://localhost:8000/llm/models/switch \
  -H "Content-Type: application/json" \
  -d '{"model": "mistral"}'
```

## ğŸ¤– Next Steps: Focus on Agents!

Now you can focus on what you want to learn: **CrewAI and LangGraph**

### CrewAI Implementation Ideas

Create `app/agents/crew_example.py`:

```python
from crewai import Agent, Task, Crew

# Example: Document analysis crew
analyzer = Agent(
    role='Content Analyzer',
    goal='Analyze text and extract key insights',
    backstory='Expert at understanding and summarizing content',
    llm=your_ollama_client
)

summarizer = Agent(
    role='Summarizer',
    goal='Create concise summaries',
    backstory='Skilled at distilling information',
    llm=your_ollama_client
)

# Define tasks and create crew
# ...
```

### LangGraph Implementation Ideas

Create `app/agents/langgraph_example.py`:

```python
from langgraph.graph import StateGraph, END

# Example: Text processing pipeline
def analyze_node(state):
    # Your logic
    pass

def summarize_node(state):
    # Your logic
    pass

# Build the graph
workflow = StateGraph(dict)
workflow.add_node("analyze", analyze_node)
workflow.add_node("summarize", summarize_node)
# ...
```

## ğŸ“ Suggested Learning Path

1. **Start Simple**: Create a 2-agent CrewAI workflow
   - One agent analyzes text
   - One agent summarizes findings
   - Connect to your OllamaClient

2. **Add API Endpoint**: Create `/agents/analyze` endpoint
   - Accepts text input
   - Runs your crew
   - Returns results

3. **Try LangGraph**: Build equivalent workflow
   - Same functionality as CrewAI version
   - Compare the developer experience

4. **Compare Frameworks**:
   - Which is easier to understand?
   - Which gives more control?
   - Which fits your use case better?

## ğŸ” Project Structure

```
crewai-playground/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # âœ… Ready
â”‚   â”œâ”€â”€ utils.py                # âœ… Ready
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py          # âœ… Ready
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ health.py           # âœ… Ready
â”‚   â”‚   â””â”€â”€ llm.py              # âœ… Ready
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ ollama_client.py    # âœ… Ready
â”‚   â””â”€â”€ agents/                 # ğŸ‘ˆ Create this for your agent code!
â”‚       â”œâ”€â”€ crew_example.py     # Your CrewAI experiments
â”‚       â””â”€â”€ langgraph_example.py # Your LangGraph experiments
â”œâ”€â”€ config.yaml                 # âœ… Ready
â”œâ”€â”€ run.py                      # âœ… Ready to use
â””â”€â”€ README.md
```

## ğŸ› Troubleshooting

**Import errors?**
- Make sure you're running from project root
- Use `python run.py` not `python app/main.py`

**Ollama connection errors?**
- Check Ollama is running: `ollama ps`
- Verify config.yaml has correct base_url
- Make sure at least one model is pulled

**Config not found?**
- config.yaml must be in project root
- Check the path in load_config() calls

## ğŸ’¡ Tips

- The FastAPI docs at `/docs` are your friend - use them!
- Check the logs for debugging info
- Start with simple 2-agent workflows
- Compare CrewAI vs LangGraph side-by-side

**You're all set! Focus on learning agents now.** ğŸ‰
